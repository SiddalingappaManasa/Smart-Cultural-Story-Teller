{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üõ†Ô∏è Environment Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "901434d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (6.3.0)\n",
            "Requirement already satisfied: langchain in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (1.2.3)\n",
            "Requirement already satisfied: langchain-community in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (0.4.1)\n",
            "Requirement already satisfied: pyttsx3 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (2.99)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (1.2.1)\n",
            "Requirement already satisfied: requests in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (2.32.5)\n",
            "Requirement already satisfied: pypdf in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (6.6.0)\n",
            "Requirement already satisfied: langchain-groq in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: huggingface_hub in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (0.36.0)\n",
            "Requirement already satisfied: mediapipe in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (0.10.31)\n",
            "Requirement already satisfied: opencv-python in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (4.12.0.88)\n",
            "Requirement already satisfied: pywin32 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (311)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (0.128.0)\n",
            "Requirement already satisfied: ffmpy in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==2.0.3 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: groovy~=0.1 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (2.2.6)\n",
            "Requirement already satisfied: orjson~=3.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (3.11.5)\n",
            "Requirement already satisfied: packaging in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (2.3.3)\n",
            "Requirement already satisfied: pillow<13.0,>=8.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (12.1.0)\n",
            "Requirement already satisfied: pydantic<=3.0,>=2.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (2.12.5)\n",
            "Requirement already satisfied: pydub in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (0.0.21)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio) (0.40.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from gradio-client==2.0.3->gradio) (2026.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain) (1.2.7)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain-community) (2.0.45)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain-community) (0.6.2)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: comtypes in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from pyttsx3) (1.4.14)\n",
            "Requirement already satisfied: pypiwin32 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from pyttsx3) (223)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: groq<1.0.0,>=0.30.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain-groq) (0.37.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from huggingface_hub) (3.20.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: absl-py~=2.3 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from mediapipe) (2.3.1)\n",
            "Requirement already satisfied: sounddevice~=0.5 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: flatbuffers~=25.9 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from mediapipe) (25.12.19)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.13.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.2)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: CFFI>=1.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from sounddevice~=0.5->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from CFFI>=1.0->sounddevice~=0.5->mediapipe) (2.23)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sriha\\internship\\storyteller\\.venv_311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install gradio langchain langchain-community pyttsx3 python-dotenv requests pypdf langchain-groq huggingface_hub mediapipe opencv-python pywin32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "85dd28fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\sriha\\internship\\storyteller\\.venv_311\\Scripts\\python.exe: No module named pywin32_postinstall\n"
          ]
        }
      ],
      "source": [
        "!python -m pywin32_postinstall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Working Directory: c:\\Users\\sriha\\internship\\storyteller\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Ensure current directory is in path for any relative file access needs\n",
        "current_dir = os.getcwd()\n",
        "if current_dir not in sys.path:\n",
        "    sys.path.append(current_dir)\n",
        "\n",
        "print(f\"Current Working Directory: {current_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Centralized Logging & CLI Formatting\n",
        "\n",
        "This section configures structured logging, log levels, and clean terminal output formatting for the entire application.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def setup_logger():\n",
        "    \"\"\"\n",
        "    Configures a professional, clean logger for the application.\n",
        "    Suppresses noisy libraries and handles known benign errors.\n",
        "    \"\"\"\n",
        "    # 1. Environment & Library Suppression\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL only for TensorFlow\n",
        "    \n",
        "    # Check for DEBUG mode\n",
        "    debug_mode = os.getenv(\"DEBUG\", \"false\").lower() == \"true\"\n",
        "    log_level = logging.DEBUG if debug_mode else logging.INFO\n",
        "\n",
        "    # 2. Configure Main Logger\n",
        "    logger = logging.getLogger(\"storyteller\")\n",
        "    logger.setLevel(log_level)\n",
        "    \n",
        "    # Clean Format\n",
        "    formatter = logging.Formatter('[%(levelname)s] %(message)s')\n",
        "    \n",
        "    handler = logging.StreamHandler(sys.stdout)\n",
        "    handler.setFormatter(formatter)\n",
        "    \n",
        "    # Avoid duplicate handlers\n",
        "    if not logger.handlers:\n",
        "        logger.addHandler(handler)\n",
        "\n",
        "    # 3. Suppress Noisy Libraries\n",
        "    # These libraries are very chatty; silence them unless critical\n",
        "    noisy_libs = [\n",
        "        \"mediapipe\", \"tensorflow\", \"absl\", \"h5py\", \n",
        "        \"numexpr\", \"urllib3\", \"httpx\", \"httpcore\"\n",
        "    ]\n",
        "    for lib in noisy_libs:\n",
        "        logging.getLogger(lib).setLevel(logging.ERROR)\n",
        "\n",
        "    # 4. Handle Asyncio Noise (WinError 10054)\n",
        "    # This error often spams on Windows/Gradio shutdown or reload\n",
        "    asyncio_logger = logging.getLogger(\"asyncio\")\n",
        "    asyncio_logger.setLevel(logging.CRITICAL) \n",
        "\n",
        "    return logger\n",
        "\n",
        "def get_logger():\n",
        "    return logging.getLogger(\"storyteller\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration & Environment Settings\n",
        "\n",
        "This section defines global constants, model selections, API settings, and environment-level configuration used across the storytelling system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# LLM Models\n",
        "MODEL_CREATIVE = \"llama-3.3-70b-versatile\"\n",
        "MODEL_FAST = \"llama-3.3-70b-versatile\" # 8b model caused panic/instability\n",
        "\n",
        "# Limits\n",
        "MAX_HISTORY_TURNS = 10\n",
        "MORAL_SCORE_MIN = -10\n",
        "MORAL_SCORE_MAX = 10\n",
        "\n",
        "# Defaults\n",
        "DEFAULT_LANGUAGE = \"English\"\n",
        "\n",
        "# Defaults\n",
        "DEFAULT_LANGUAGE = \"English\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Asset Download & Setup Utility\n",
        "\n",
        "This section downloads and prepares required external model assets (e.g., MediaPipe task files) needed for local facial emotion detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model from https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task...\n",
            "Success: Saved to c:\\Users\\sriha\\internship\\storyteller\\face_landmarker.task\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "url = \"https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\"\n",
        "output_path = \"face_landmarker.task\"\n",
        "\n",
        "print(f\"Downloading model from {url}...\")\n",
        "try:\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    with open(output_path, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "    print(f\"Success: Saved to {os.path.abspath(output_path)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Download failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cinematic Prompt & Visual Framing Engine\n",
        "\n",
        "This section translates narrative and emotional context into cinematic descriptors such as camera angle, lighting, and atmosphere.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\sriha\\internship\\storyteller\\.venv_311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "# from config import MODEL_FAST  # Cell-local import\n",
        "# from logger_config import get_logger  # Cell-local import\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "class CinematographyEngine:\n",
        "    def __init__(self):\n",
        "        # We use a specialized instance for visual instruction\n",
        "        self.llm = ChatGroq(model=MODEL_FAST, temperature=0.7)\n",
        "\n",
        "    def enhance_prompt(self, story_segment, emotion):\n",
        "        \"\"\"\n",
        "        Generates a visually rich, cinematic description using an LLM.\n",
        "        Avoids hardcoded mappings.\n",
        "        \"\"\"\n",
        "        system_instruction = (\n",
        "            \"You are an expert Virtual Cinematographer and Art Director. \"\n",
        "            \"Your task is to translate a story segment and an emotion into a precise \"\n",
        "            \"visual description for AI Image Generation (Stable Diffusion/Flux). \"\n",
        "            \"Focus ONLY on: Camera Angle, Lighting, Color Palette, Depth of Field, and Composition. \"\n",
        "            \"Do NOT include the story action itself, just the visual style keywords. \"\n",
        "            \"Keep it comma-separated and under 40 words.\"\n",
        "        )\n",
        "\n",
        "        prompt_template = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", system_instruction),\n",
        "            (\"human\", \"Story: {story}\\nEmotion: {emotion}\\n\\nVisual Keywords:\")\n",
        "        ])\n",
        "\n",
        "        chain = prompt_template | self.llm\n",
        "        \n",
        "        try:\n",
        "            result = chain.invoke({\"story\": story_segment, \"emotion\": emotion})\n",
        "            return result.content.strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Cinematography Engine Error: {e}\")\n",
        "            # Fallback if LLM fails\n",
        "            return f\"cinematic shot, {emotion} lighting, 8k resolution\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Cultural Context Generator\n",
        "\n",
        "This section implements a fully generative culture engine that recalls culturally authentic concepts, values, symbols, and terminology on demand using an LLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from config import MODEL_FAST  # Cell-local import\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "# from logger_config import get_logger  # Cell-local import\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "class CultureEngine:\n",
        "    def __init__(self):\n",
        "        # Use Fast model for quick context retrieval/generation\n",
        "        self.llm = ChatGroq(model=MODEL_FAST)\n",
        "\n",
        "    def get_context_string(self, theme):\n",
        "        \"\"\"\n",
        "        Dynamically generates a 'Knowledge Block' about the theme using the LLM.\n",
        "        \"\"\"\n",
        "        if not theme:\n",
        "            return \"\"\n",
        "\n",
        "        logger.info(f\"CultureEngine: Generatively recalling facts for '{theme}'...\")\n",
        "        \n",
        "        system_prompt = (\n",
        "            \"You are an expert Cultural Anthropologist and Mythologist with encyclopedic knowledge of world cultures, \"\n",
        "            \"folklore, and history. Your goal is to provide a concise, factual, and authentic 'Knowledge Block' \"\n",
        "            \"that a storyteller can use to ground their narrative.\"\n",
        "        )\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"Topic: {theme}\\n\\n\"\n",
        "            \"Provide 9-10 key authentic cultural elements, including:\\n\"\n",
        "            \"1. Specific terminology (greetings, clothing, weapons, tools)\\n\"\n",
        "            \"2. Key festivals or rituals\\n\"\n",
        "            \"3. Mythological figures or legends\\n\"\n",
        "            \"4. Social hierarchy or values\\n\\n\"\n",
        "            \"Format: A concise list or paragraph. Strictly factual and authentic. No preamble.\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            messages = [\n",
        "                SystemMessage(content=system_prompt),\n",
        "                HumanMessage(content=user_prompt)\n",
        "            ]\n",
        "            response = self.llm.invoke(messages)\n",
        "            return response.content\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Culture Generation Failed: {e}\")\n",
        "            return \"General cultural knowledge applies.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ce = CultureEngine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Facial Emotion Detection Engine\n",
        "\n",
        "This section detects user facial expressions in real time using MediaPipe and maps them to high-level emotional states for narrative tone adaptation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "import numpy as np\n",
        "import os\n",
        "# from logger_config import get_logger  # Cell-local import\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "class EmotionEngine:\n",
        "    def __init__(self, model_path=\"face_landmarker.task\"):\n",
        "        \"\"\"\n",
        "        Initializes the MediaPipe FaceLandmarker with Blendshapes enabled.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(model_path):\n",
        "                logger.warning(f\"Model file '{model_path}' not found. Emotion detection disabled.\")\n",
        "                self.detector = None\n",
        "                return\n",
        "\n",
        "            base_options = python.BaseOptions(model_asset_path=model_path)\n",
        "            options = vision.FaceLandmarkerOptions(\n",
        "                base_options=base_options,\n",
        "                output_face_blendshapes=True,\n",
        "                output_facial_transformation_matrixes=False,\n",
        "                num_faces=1\n",
        "            )\n",
        "            self.detector = vision.FaceLandmarker.create_from_options(options)\n",
        "            logger.info(\"EmotionEngine (FaceLandmarker) initialized successfully.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing EmotionEngine: {e}\")\n",
        "            self.detector = None\n",
        "\n",
        "    def detect_emotion(self, image):\n",
        "        \"\"\"\n",
        "        Detects emotion from a numpy image (RGB) using Blendshapes.\n",
        "        Returns: { \"emotion\": label, \"confidence\": float }\n",
        "        \"\"\"\n",
        "        if self.detector is None or image is None:\n",
        "            return {\"emotion\": \"neutral\", \"confidence\": 0.0}\n",
        "\n",
        "        try:\n",
        "            # Create MP Image from Numpy\n",
        "            # Gradio provides RGB numpy array\n",
        "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
        "\n",
        "            # Detect\n",
        "            detection_result = self.detector.detect(mp_image)\n",
        "\n",
        "            if not detection_result.face_blendshapes:\n",
        "                return {\"emotion\": \"neutral\", \"confidence\": 0.0}\n",
        "\n",
        "            # Extract Blendshapes (list of categories)\n",
        "            # There is 1 face, so index 0\n",
        "            blendshapes = detection_result.face_blendshapes[0]\n",
        "            \n",
        "            # Map blendshapes to dict for easy access\n",
        "            bs = {b.category_name: b.score for b in blendshapes}\n",
        "\n",
        "            # Heuristics for Basic Emotions based on ARKit Blendshapes\n",
        "            # Scores are 0.0 to 1.0\n",
        "            \n",
        "            scores = {\n",
        "                \"happy\": (bs.get('mouthSmileLeft', 0) + bs.get('mouthSmileRight', 0)) / 2,\n",
        "                \"surprise\": (bs.get('browInnerUp', 0) + bs.get('jawOpen', 0)) / 2,\n",
        "                \"angry\": (bs.get('browDownLeft', 0) + bs.get('browDownRight', 0)) / 2,\n",
        "                \"sad\": (bs.get('mouthFrownLeft', 0) + bs.get('mouthFrownRight', 0) + bs.get('browInnerUp', 0)) / 3,\n",
        "                \"fear\": (bs.get('eyeWideLeft', 0) + bs.get('eyeWideRight', 0) + bs.get('mouthStretchLeft', 0)) / 3\n",
        "            }\n",
        "\n",
        "            # Find max score\n",
        "            best_emotion = max(scores, key=scores.get)\n",
        "            best_score = scores[best_emotion]\n",
        "\n",
        "            # Thresholding\n",
        "            if best_score < 0.3: # If signals are weak, default to neutral\n",
        "                return {\"emotion\": \"neutral\", \"confidence\": round(1.0 - best_score, 2)}\n",
        "            \n",
        "            logger.info(f\"Detected emotion: {best_emotion} (confidence={best_score:.2f})\")\n",
        "            \n",
        "            return {\n",
        "                \"emotion\": best_emotion,\n",
        "                \"confidence\": round(best_score, 2)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Emotion detection runtime error: {e}\")\n",
        "            return {\"emotion\": \"neutral\", \"confidence\": 0.0}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Moral Evaluation & Karma System\n",
        "\n",
        "This section evaluates user choices across ethical dimensions such as compassion, courage, and greed, updating the moral trajectory of the story.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "# from config import MODEL_FAST, MORAL_SCORE_MIN, MORAL_SCORE_MAX  # Cell-local import\n",
        "# from logger_config import get_logger  # Cell-local import\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "class MoralScore(BaseModel):\n",
        "    compassion: int = Field(description=\"Change in compassion score (-5 to +5)\")\n",
        "    courage: int = Field(description=\"Change in courage score (-5 to +5)\")\n",
        "    greed: int = Field(description=\"Change in greed score (-5 to +5)\")\n",
        "    reasoning: str = Field(description=\"Brief reason for the score\")\n",
        "\n",
        "class MoralEngine:\n",
        "    def __init__(self):\n",
        "        self.llm = ChatGroq(model=MODEL_FAST, temperature=0.5)\n",
        "        self.scores = {\"compassion\": 0, \"courage\": 0, \"greed\": 0}\n",
        "        \n",
        "        self.parser = JsonOutputParser(pydantic_object=MoralScore)\n",
        "\n",
        "    def score_choice(self, user_choice, story_context):\n",
        "        \"\"\"Evaluates the user's last choice.\"\"\"\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are a Moral Arbiter in a story game. Analyze the user's choice and assign score changes.\"),\n",
        "            (\"human\", \"Story Context: {context}\\nUser Choice: {choice}\\n\\n{format_instructions}\")\n",
        "        ])\n",
        "\n",
        "        chain = prompt | self.llm | self.parser\n",
        "        \n",
        "        try:\n",
        "            result = chain.invoke({\n",
        "                \"context\": story_context[-500:], # Last 500 chars context\n",
        "                \"choice\": user_choice,\n",
        "                \"format_instructions\": self.parser.get_format_instructions()\n",
        "            })\n",
        "            \n",
        "            # Update internal state with clamping\n",
        "            for trait in [\"compassion\", \"courage\", \"greed\"]:\n",
        "                change = result.get(trait, 0)\n",
        "                new_score = self.scores[trait] + change\n",
        "                # Clamp score\n",
        "                self.scores[trait] = max(MORAL_SCORE_MIN, min(MORAL_SCORE_MAX, new_score))\n",
        "            \n",
        "            return result\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Moral Engine Error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_reflection(self):\n",
        "        \"\"\"Generates a final moral summary.\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Based on these final scores: {self.scores},\n",
        "        write a 2-sentence spiritual reflection for the player, referencing concepts like Karma or Dharma if appropriate.\n",
        "        \"\"\"\n",
        "        response = self.llm.invoke(prompt)\n",
        "        return response.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Character Identity Engine\n",
        "\n",
        "This section generates culturally grounded protagonist identities, including names, traits, and background context, based on the selected theme.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class Character:\n",
        "    def __init__(self, name, culture, age=20, traits=None, voice_id=\"21m00Tcm4TlvDq8ikWAM\", face_seed=None):\n",
        "        self.id = f\"char_{random.randint(1000, 9999)}\"\n",
        "        self.name = name\n",
        "        self.culture = culture\n",
        "        self.age = age\n",
        "        self.traits = traits if traits else []\n",
        "        self.voice_id = voice_id\n",
        "        # Persistent seed for image generation consistency\n",
        "        self.face_seed = face_seed if face_seed else random.randint(10000, 99999)\n",
        "        \n",
        "    def add_trait(self, trait):\n",
        "        if trait not in self.traits:\n",
        "            self.traits.append(trait)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            \"id\": self.id,\n",
        "            \"name\": self.name,\n",
        "            \"culture\": self.culture,\n",
        "            \"age\": self.age,\n",
        "            \"traits\": self.traits,\n",
        "            \"voice_id\": self.voice_id,\n",
        "            \"face_seed\": self.face_seed\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def from_dict(data):\n",
        "        return Character(\n",
        "            name=data[\"name\"],\n",
        "            culture=data[\"culture\"],\n",
        "            age=data[\"age\"],\n",
        "            traits=data[\"traits\"],\n",
        "            voice_id=data[\"voice_id\"],\n",
        "            face_seed=data[\"face_seed\"]\n",
        "        )\n",
        "\n",
        "# from config import MODEL_FAST  # Cell-local import\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "# from logger_config import get_logger  # Cell-local import\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "class CharacterIdentity(BaseModel):\n",
        "    name: str = Field(description=\"A culturally appropriate name for the protagonist\")\n",
        "    culture_label: str = Field(description=\"A formally normalized culture label (e.g., 'Indian Epic - Ramayana')\")\n",
        "\n",
        "class CharacterEngine:\n",
        "    def __init__(self):\n",
        "        self.llm = ChatGroq(model=MODEL_FAST)\n",
        "        self.parser = JsonOutputParser(pydantic_object=CharacterIdentity)\n",
        "\n",
        "    def _generate_identity_llm(self, theme_input):\n",
        "        \"\"\"Generates dynamic character identity using LLM.\"\"\"\n",
        "        try:\n",
        "            prompt = (\n",
        "                f\"Analyze the theme '{theme_input}'.\\n\"\n",
        "                \"Generate a culturally authentic protagonist name and a formal culture label.\\n\"\n",
        "                \"Example: 'samurai' -> Name: 'Kenji', Culture: 'Japanese History - Samurai Era'\\n\"\n",
        "                f\"{self.parser.get_format_instructions()}\"\n",
        "            )\n",
        "            response = self.llm.invoke(prompt)\n",
        "            data = self.parser.parse(response.content)\n",
        "            return data[\"name\"], data[\"culture_label\"]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Identity Generation Failed: {e}\")\n",
        "            return \"Protagonist\", theme_input.title()\n",
        "\n",
        "    def initialize_character(self, name, culture_input):\n",
        "        # If name is generic or missing, use LLM to generate identity\n",
        "        if not name or \"Protagonist\" in name:\n",
        "            gen_name, gen_culture = self._generate_identity_llm(culture_input)\n",
        "            final_name = gen_name\n",
        "            final_culture = gen_culture\n",
        "        else:\n",
        "            final_name = name\n",
        "            final_culture = culture_input.title()\n",
        "            \n",
        "        return Character(final_name, final_culture)\n",
        "\n",
        "    def get_visual_description(self, character):\n",
        "        \"\"\"Returns a stable visual description for the image prompt.\"\"\"\n",
        "        traits_str = \", \".join(character.traits)\n",
        "        return (\n",
        "            f\"character {character.name}, {character.culture} ethnicity, \"\n",
        "            f\"age {character.age}, wearing traditional attire, {traits_str}, \"\n",
        "            f\"consistent face, high detail\"\n",
        "        )\n",
        "\n",
        "    def update_traits_from_scores(self, character, scores):\n",
        "        \"\"\"Updates character traits based on moral alignment.\"\"\"\n",
        "        new_traits = []\n",
        "        \n",
        "        # Compassion\n",
        "        c_score = scores.get('compassion', 0)\n",
        "        if c_score >= 5:\n",
        "            new_traits.append(\"Kind\")\n",
        "        elif c_score <= -5:\n",
        "            new_traits.append(\"Ruthless\")\n",
        "            \n",
        "        # Courage\n",
        "        co_score = scores.get('courage', 0)\n",
        "        if co_score >= 5:\n",
        "            new_traits.append(\"Brave\")\n",
        "        elif co_score <= -5:\n",
        "            new_traits.append(\"Cowardly\")\n",
        "            \n",
        "        # Greed\n",
        "        g_score = scores.get('greed', 0)\n",
        "        if g_score >= 5:\n",
        "            new_traits.append(\"Ambitious\")\n",
        "        elif g_score <= -5:\n",
        "            new_traits.append(\"Generous\")\n",
        "            \n",
        "        for trait in new_traits:\n",
        "            character.add_trait(trait)\n",
        "        \n",
        "        return character\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Media Generation & Narration Engine\n",
        "\n",
        "This section handles optional audio narration and image generation, with graceful fallbacks when media capabilities are unavailable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import traceback\n",
        "import requests\n",
        "import base64\n",
        "import pyttsx3\n",
        "# from cinematography_engine import CinematographyEngine  # Cell-local import\n",
        "# from logger_config import get_logger  # Cell-local import\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "class MediaEngine:\n",
        "    def __init__(self):\n",
        "        # Hugging Face token check\n",
        "        self.hf_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
        "        if not self.hf_token:\n",
        "            logger.warning(\"HUGGINGFACE_API_TOKEN not found. Image generation disabled.\")\n",
        "\n",
        "        # pyttsx3 is initialized per-call to avoid threading issues on Windows/Gradio\n",
        "\n",
        "        # Initialize Cinematography Engine\n",
        "        try:\n",
        "            self.cine_engine = CinematographyEngine()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to init CinematographyEngine: {e}\")\n",
        "            self.cine_engine = None\n",
        "\n",
        "    # ---------------- IMAGE GENERATION ----------------\n",
        "    def generate_scene(self, story_text, emotion=\"neutral\", character_desc=\"\", visual_keywords_bypass=None):\n",
        "        if not self.hf_token:\n",
        "            return None, \"image\"\n",
        "\n",
        "        try:\n",
        "            # 1. Get Cinematography Keywords\n",
        "            if visual_keywords_bypass:\n",
        "                keywords = visual_keywords_bypass\n",
        "            elif self.cine_engine:\n",
        "                keywords = self.cine_engine.enhance_prompt(story_text[:500], emotion)\n",
        "            else:\n",
        "                keywords = f\"cinematic, {emotion} atmosphere\"\n",
        "\n",
        "            # 2. Construct Prompt with Character Consistency\n",
        "            prompt = f\"\"\"\n",
        "            masterpiece, ultra-detailed cinematic illustration, storybook fantasy art,\n",
        "            authentic cultural aesthetics, rich textures, 8k resolution,\n",
        "            {keywords},\n",
        "            \n",
        "            SCENE:\n",
        "            {story_text[:400]}\n",
        "            \n",
        "            CHARACTER FOCUS:\n",
        "            {character_desc}\n",
        "            \n",
        "            STYLE:\n",
        "            digital painting, concept art, unreal engine quality, artstation trending\n",
        "            \n",
        "            NEGATIVE:\n",
        "            blurry, low resolution, distorted face, extra limbs, bad anatomy, watermark, text\n",
        "            \"\"\"\n",
        "\n",
        "            API_URL = (\n",
        "                \"https://router.huggingface.co/hf-inference/models/\"\n",
        "                \"black-forest-labs/FLUX.1-schnell\"\n",
        "            )\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {self.hf_token}\",\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            }\n",
        "\n",
        "            response = requests.post(\n",
        "                API_URL,\n",
        "                headers=headers,\n",
        "                json={\"inputs\": prompt},\n",
        "                timeout=60\n",
        "            )\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                raise Exception(f\"HF Error {response.status_code}: {response.text}\")\n",
        "\n",
        "            filename = f\"scene_{int(time.time())}.png\"\n",
        "            output_path = os.path.abspath(filename)\n",
        "\n",
        "            with open(output_path, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "\n",
        "            logger.info(f\"Image saved ‚Üí {output_path}\")\n",
        "            return output_path, \"image\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Image generation failed: {e}\")\n",
        "            logger.debug(traceback.format_exc())\n",
        "\n",
        "            with open(\"debug_error.log\", \"w\") as f:\n",
        "                f.write(str(e))\n",
        "\n",
        "            return None, \"image\"\n",
        "\n",
        "    # ---------------- AUDIO GENERATION ----------------\n",
        "    def generate_audio(self, text, voice_id=None):\n",
        "        try:\n",
        "            # Re-initialize pyttsx3 per call for thread safety in Gradio\n",
        "            engine = pyttsx3.init()\n",
        "            # Optional: Set properties\n",
        "            engine.setProperty('rate', 150)\n",
        "            engine.setProperty('volume', 1.0)\n",
        "            \n",
        "            output_path = f\"audio_{int(time.time())}.mp3\"\n",
        "            abs_output_path = os.path.abspath(output_path)\n",
        "            \n",
        "            # Saving to file\n",
        "            engine.save_to_file(text, abs_output_path)\n",
        "            engine.runAndWait()\n",
        "            \n",
        "            # Explicit cleanup if possible (pyttsx3 doesn't have a close(), but letting it go out of scope helps)\n",
        "            del engine\n",
        "\n",
        "            return abs_output_path\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Audio generation failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    # ---------------- VIDEO GENERATION ----------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Narrative Generation Engine\n",
        "\n",
        "This section contains the main storytelling logic, combining cultural context, character identity, emotion, and moral state to generate adaptive narratives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from config import MODEL_CREATIVE, MAX_HISTORY_TURNS  # Cell-local import\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "# from culture_engine import CultureEngine  # Cell-local import\n",
        "# from logger_config import get_logger  # Cell-local import\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "class StoryOutput(BaseModel):\n",
        "    story_text: str = Field(description=\"The narrative content (100-150 words) with choices at the end\")\n",
        "    emotion: str = Field(description=\"One word emotion: joy, sadness, anger, fear, peace, mystery\")\n",
        "    visual_keywords: str = Field(description=\"Comma-separated visual keywords(Camera Angle, Lighting, Color Palette)\")\n",
        "\n",
        "class StoryTeller:\n",
        "    def __init__(self):\n",
        "        self.llm = ChatGroq(model=MODEL_CREATIVE)\n",
        "        self.history = []\n",
        "        self.culture_engine = CultureEngine()\n",
        "        self.language_instruction = \"Narrate in English.\"\n",
        "        self.parser = JsonOutputParser(pydantic_object=StoryOutput)\n",
        "\n",
        "    def set_language(self, language=\"English\"):\n",
        "        if language and language.lower() != \"english\":\n",
        "            self.language_instruction = (\n",
        "                f\"Narrate primarily in English, BUT you MUST adhere to the following code-switching rules:\\n\"\n",
        "                f\"1. Use {language} for ALL opening greetings and significant cultural terms.\\n\"\n",
        "                f\"2. Quotes and dialogue MUST be in {language} (provide English translation in parentheses if long).\\n\"\n",
        "                f\"3. Ensure the tone reflects the linguistic nuance of {language}.\\n\"\n",
        "                f\"Example: 'Namaste! (Hello!) The wind howled...' \"\n",
        "            )\n",
        "        else:\n",
        "            self.language_instruction = \"Narrate in English.\"\n",
        "\n",
        "    def _trim_history(self):\n",
        "        \"\"\"Trims history to keep only the most recent interactions.\"\"\"\n",
        "        try:\n",
        "            # Keep system prompt (index 0) + last N turns\n",
        "            if len(self.history) > MAX_HISTORY_TURNS:\n",
        "                # Safely slice the last (N-1) elements\n",
        "                keep_count = MAX_HISTORY_TURNS - 1\n",
        "                recent_history = self.history[-keep_count:]\n",
        "                self.history = [self.history[0]] + recent_history\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"History Trimming Error: {e}\")\n",
        "            # Fallback: Just keep explicit last 5\n",
        "            if len(self.history) > 5:\n",
        "                self.history = [self.history[0]] + self.history[-5:]\n",
        "\n",
        "    def start_story(self, theme, language=\"English\"):\n",
        "        \"\"\"Initializes the story based on a theme and cultural context.\"\"\"\n",
        "        self.set_language(language)\n",
        "        \n",
        "        # 1. Retrieve Cultural Context (RAG)\n",
        "        logger.info(f\"Retrieving cultural context for: {theme}\")\n",
        "        context_str = self.culture_engine.get_context_string(theme)\n",
        "        \n",
        "        if context_str:\n",
        "            grounding_instruction = (\n",
        "                \"You have access to the following trusted cultural knowledge:\\n\"\n",
        "                f\"{context_str}\\n\\n\"\n",
        "                \"CRITICAL INSTRUCTION: You MUST ground your story in this provided context. \"\n",
        "                \"Use specific symbols, names, and festivals mentioned. \"\n",
        "                \"Do NOT hallunicate details if they contradict this context.\"\n",
        "            )\n",
        "        else:\n",
        "            grounding_instruction = \"No specific cultural documents found. Rely on general knowledge but remain respectful and authentic.\"\n",
        "\n",
        "        # 2. Build System Prompt\n",
        "        system_prompt = (\n",
        "            \"You are a 'Smart Cultural Storyteller'. Your goal is to preserve and retell cultural narratives \"\n",
        "            \"in an engaging, interactive 'choose-your-own-adventure' style.\\n\\n\"\n",
        "            f\"{grounding_instruction}\\n\\n\"\n",
        "            f\"Language Rule: {self.language_instruction}\\n\\n\"\n",
        "            \"Format:\\n\"\n",
        "            \"- Keep responses concise (100-150 words).\\n\"\n",
        "            \"- End with exactly 2 or 3 distinct choices.\\n\"\n",
        "            \"- CRITICAL: Format choices as numbered list: 1. [Choice A] 2. [Choice B] etc.\\n\"\n",
        "            \"- If information is unknown, acknowledge it subtly or steer towards known elements.\\n\"\n",
        "            \"OUTPUT JSON ONLY: Return a valid JSON object with keys: 'story_text', 'emotion', 'visual_keywords'.\"\n",
        "        )\n",
        "\n",
        "        self.history = [SystemMessage(content=system_prompt)]\n",
        "        \n",
        "        prompt = f\"Start a story about {theme}. Set the scene and offer numbered choices.\\n{self.parser.get_format_instructions()}\"\n",
        "        self.history.append(HumanMessage(content=prompt))\n",
        "        \n",
        "        try:\n",
        "            response = self.llm.invoke(self.history)\n",
        "            self.history.append(response)\n",
        "            parsed_response = self.parser.parse(response.content)\n",
        "            return parsed_response\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Story Start Error: {e}\")\n",
        "            # Fallback\n",
        "            return {\n",
        "                \"story_text\": f\"The story begins with {theme}. (Error generating full story)\",\n",
        "                \"emotion\": \"mystery\",\n",
        "                \"visual_keywords\": \"foggy, ancient, mysterious\"\n",
        "            }\n",
        "\n",
        "    def continue_story(self, user_choice):\n",
        "        \"\"\"Continues the story based on user's choice.\"\"\"\n",
        "        self._trim_history()\n",
        "        self.history.append(HumanMessage(content=user_choice))\n",
        "        \n",
        "        try:\n",
        "            response = self.llm.invoke(self.history)\n",
        "            self.history.append(response)\n",
        "            parsed_response = self.parser.parse(response.content)\n",
        "            return parsed_response\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Story Continue Error: {e}\")\n",
        "            return {\n",
        "                \"story_text\": \"The story continues... (Error generating segment)\",\n",
        "                \"emotion\": \"neutral\",\n",
        "                \"visual_keywords\": \"standard scene\"\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Application Orchestration & Interactive UI\n",
        "\n",
        "This section serves as the main entry point, orchestrating all engines and launching the interactive Gradio-based storytelling interface.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] ---------------------------------------------------------------\n",
            "[INFO]    üìñ SMART CULTURAL STORYTELLER 2.0\n",
            "[INFO]    Research-Grade Engine | Python 3.11 | Groq | MediaPipe\n",
            "[INFO] ---------------------------------------------------------------\n",
            "[INFO] System initializing...\n",
            "[INFO] EmotionEngine (FaceLandmarker) initialized successfully.\n",
            "[INFO] Engine validation complete. Launching UI...\n",
            "[INFO] Starting Web Server at http://127.0.0.1:7860...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Retrieving cultural context for: japan\n",
            "[INFO] CultureEngine: Generatively recalling facts for 'japan'...\n",
            "[ERROR] Image generation failed: HF Error 402: {\"error\":\"You have reached the free monthly usage limit for hf-inference. Subscribe to PRO to get 20x more included usage, or add pre-paid credits to your account.\"}\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "# from logger_config import setup_logger, get_logger  # Cell-local import\n",
        "\n",
        "# 1. Setup Professional Logging (Must be first)\n",
        "setup_logger()\n",
        "logger = get_logger()\n",
        "\n",
        "# 2. Startup Banner\n",
        "logger.info(\"---------------------------------------------------------------\")\n",
        "logger.info(\"   üìñ SMART CULTURAL STORYTELLER 2.0\")\n",
        "logger.info(\"   Research-Grade Engine | Python 3.11 | Groq | MediaPipe\")\n",
        "logger.info(\"---------------------------------------------------------------\")\n",
        "logger.info(\"System initializing...\")\n",
        "\n",
        "# from story_engine import StoryTeller  # Cell-local import\n",
        "# from media_engine import MediaEngine  # Cell-local import\n",
        "# from character_engine import CharacterEngine, Character  # Cell-local import\n",
        "# from moral_engine import MoralEngine  # Cell-local import\n",
        "# from emotion_engine import EmotionEngine  # Cell-local import\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize Engines\n",
        "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
        "    logger.error(\"GOOGLE_API_KEY not found.\")\n",
        "    exit(1)\n",
        "\n",
        "story_teller = StoryTeller()\n",
        "media_engine = MediaEngine()\n",
        "character_engine = CharacterEngine()\n",
        "try:\n",
        "    emotion_engine = EmotionEngine()\n",
        "    if emotion_engine.detector is None:\n",
        "         logger.warning(\"Emotion Engine initialized but detector is None.\")\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Emotion Engine failed to load ({e}). Face detection disabled.\")\n",
        "    emotion_engine = None\n",
        "\n",
        "logger.info(\"Engine validation complete. Launching UI...\")\n",
        "\n",
        "def start_story_handler(theme, language, history_state):\n",
        "    try:\n",
        "        if not theme:\n",
        "            yield \"Please enter a theme.\", None, None, history_state, \"\", \"\"\n",
        "            return\n",
        "        \n",
        "        # Initialize Character & Moral Engine\n",
        "        char_name = f\"Protagonist_{theme.split()[0]}\"\n",
        "        character = character_engine.initialize_character(char_name, theme)\n",
        "        moral = MoralEngine()\n",
        "\n",
        "        # Start Story (Returns JSON dict)\n",
        "        story_data = story_teller.start_story(theme, language)\n",
        "        story_text = story_data.get(\"story_text\", \"\")\n",
        "        emotion = story_data.get(\"emotion\", \"neutral\")\n",
        "        visual_keywords = story_data.get(\"visual_keywords\")\n",
        "        \n",
        "        # Immediate yield: Story Text\n",
        "        yield story_text, None, None, {\n",
        "            \"character\": character.to_dict(), \n",
        "            \"moral_scores\": moral.scores\n",
        "        }, \"Compassion: 0 | Courage: 0 | Greed: 0\", \"\"\n",
        "\n",
        "        # Generate Audio\n",
        "        audio = media_engine.generate_audio(story_text)\n",
        "        yield story_text, audio, None, {\n",
        "            \"character\": character.to_dict(), \n",
        "            \"moral_scores\": moral.scores\n",
        "        }, \"Compassion: 0 | Courage: 0 | Greed: 0\", \"\"\n",
        "\n",
        "        # Generate Image (Optimized)\n",
        "        char_desc = character_engine.get_visual_description(character)\n",
        "        media_path, media_type = media_engine.generate_scene(story_text, emotion, char_desc, visual_keywords_bypass=visual_keywords)\n",
        "        \n",
        "        image_update = gr.update(value=media_path, visible=True) if media_type == \"image\" else gr.update(visible=False)\n",
        "\n",
        "        yield (\n",
        "            story_text, \n",
        "            audio, \n",
        "            image_update,\n",
        "            {\n",
        "                \"character\": character.to_dict(), \n",
        "                \"moral_scores\": moral.scores\n",
        "            },\n",
        "            f\"Compassion: 0 | Courage: 0 | Greed: 0\",\n",
        "            \"\" # Status msg\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in start_story_handler: {e}\")\n",
        "        yield f\"Error: {e}\", None, None, None, \"\", \"\"\n",
        "\n",
        "\n",
        "def process_emotion_stream(image, last_time):\n",
        "    \"\"\"\n",
        "    Background process to detect emotion from stream.\n",
        "    Throttled to run every 1.0s to avoid log spam/CPU load.\n",
        "    Returns: (new_emotion_label, new_timestamp)\n",
        "    \"\"\"\n",
        "    current_time = time.time()\n",
        "    \n",
        "    # Throttle: Only process if 1.0s passed\n",
        "    if image is None or (current_time - last_time < 1.0):\n",
        "        return gr.skip(), gr.skip()\n",
        "        \n",
        "    if emotion_engine:\n",
        "        try:\n",
        "            # This calling detect_emotion will trigger the print log if confidence > 0.3\n",
        "            result = emotion_engine.detect_emotion(image)\n",
        "            label = result[\"emotion\"] if result else \"neutral\"\n",
        "            return label, current_time\n",
        "        except Exception:\n",
        "            return \"neutral\", current_time\n",
        "            \n",
        "    return \"neutral\", current_time\n",
        "\n",
        "\n",
        "def continue_story_handler(user_choice, user_emotion_label, state):\n",
        "    try:\n",
        "        if not user_choice:\n",
        "            yield \"Please make a choice.\", None, None, state, \"\", \"\"\n",
        "            return\n",
        "\n",
        "        # Rehydrate State\n",
        "        if not state:\n",
        "            yield \"Session expired. Start over.\", None, None, None, \"\", \"\"\n",
        "            return\n",
        "        \n",
        "        character = Character.from_dict(state[\"character\"])\n",
        "        moral = MoralEngine()\n",
        "        moral.scores = state[\"moral_scores\"]\n",
        "\n",
        "        # 1. Score the Choice\n",
        "        moral_result = moral.score_choice(user_choice, story_teller.history[-1].content)\n",
        "        \n",
        "        # 2. Update Character Traits\n",
        "        character = character_engine.update_traits_from_scores(character, moral.scores)\n",
        "\n",
        "        # 3. Continue Story (Returns JSON)\n",
        "        # Use the passed-in emotion label (from State)\n",
        "        \n",
        "        # Inject into context\n",
        "        context_choice = f\"{user_choice} (User Facial Emotion: {user_emotion_label})\"\n",
        "        \n",
        "        story_data = story_teller.continue_story(context_choice)\n",
        "        story_text = story_data.get(\"story_text\", \"\")\n",
        "        # Blend Emotions: Story > Facial\n",
        "        story_emotion = story_data.get(\"emotion\", \"neutral\")\n",
        "        final_emotion = story_emotion if story_emotion != \"neutral\" else user_emotion_label\n",
        "        visual_keywords = story_data.get(\"visual_keywords\")\n",
        "        \n",
        "        moral_display = f\"Compassion: {moral.scores['compassion']} | Courage: {moral.scores['courage']} | Greed: {moral.scores['greed']}\"\n",
        "        \n",
        "        status_msg = f\"‚ú® Karma Updated! (Compassion: {moral_result.get('compassion')}, Courage: {moral_result.get('courage')}, Greed: {moral_result.get('greed')}) | Face: {user_emotion_label}\"\n",
        "\n",
        "        # Yield Text immediately\n",
        "        yield story_text, None, None, state, moral_display, status_msg\n",
        "        \n",
        "        # 4. Generate Audio\n",
        "        audio = media_engine.generate_audio(story_text)\n",
        "        yield story_text, audio, None, state, moral_display, status_msg\n",
        "\n",
        "        # 5. Generate Media\n",
        "        char_desc = character_engine.get_visual_description(character)\n",
        "        media_path, media_type = media_engine.generate_scene(story_text, final_emotion, char_desc, visual_keywords_bypass=visual_keywords)\n",
        "        image_update = gr.update(value=media_path, visible=True) if media_type == \"image\" else gr.update(visible=False)\n",
        "\n",
        "        # Update State\n",
        "        state[\"moral_scores\"] = moral.scores\n",
        "        state[\"character\"] = character.to_dict()\n",
        "\n",
        "        # Check if story ended\n",
        "        if \"THE END\" in story_text.upper():\n",
        "            reflection = moral.generate_reflection()\n",
        "            story_text += f\"\\n\\n‚ú® **Moral Reflection**: {reflection}\"\n",
        "\n",
        "        yield story_text, audio, image_update, state, moral_display, status_msg\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in continue_story_handler: {e}\")\n",
        "        yield f\"Error: {e}\", None, None, state, \"\", \"\"\n",
        "\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks(title=\"Smart Cultural Storyteller\") as demo:\n",
        "    state = gr.State({})\n",
        "    # State to hold background emotion detection\n",
        "    emotion_state = gr.State(\"neutral\")\n",
        "    # State to hold last processing timestamp for throttling\n",
        "    timer_state = gr.State(0.0)\n",
        "\n",
        "    gr.Markdown(\"# üìñ Smart Cultural Storyteller\")\n",
        "    gr.Markdown(\"Research-Grade Interactive Storytelling with RAG, Cinematography, and Moral Agents.\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Input Section\n",
        "            theme_input = gr.Textbox(label=\"Culture / Theme\", placeholder=\"Indian Folklore, Samurai Legends...\")\n",
        "            lang_input = gr.Dropdown([\"English\", \"Hindi\", \"Telugu\", \"Japanese\", \"Spanish\"], label=\"Language\", value=\"English\")\n",
        "            # Enable Streaming\n",
        "            webcam_input = gr.Image(label=\"Your Emotion (Optional)\", sources=[\"webcam\"], type=\"numpy\", visible=True, streaming=True)\n",
        "            # Oral History Removed\n",
        "            \n",
        "            start_btn = gr.Button(\"Start New Journey\", variant=\"primary\")\n",
        "            \n",
        "            gr.Markdown(\"---\")\n",
        "            \n",
        "            # Game Controls\n",
        "            choice_input = gr.Textbox(label=\"Your Choice / Action\")\n",
        "            continue_btn = gr.Button(\"Make Choice\")\n",
        "\n",
        "            # Stats Display\n",
        "            gr.Markdown(\"### ‚öñÔ∏è Moral Alignment\")\n",
        "            moral_info = gr.Textbox(interactive=False, label=\"Karma Score\")\n",
        "            status_info = gr.Textbox(interactive=False, label=\"Recent Updates\")\n",
        "            \n",
        "        with gr.Column(scale=2):\n",
        "            story_display = gr.Markdown(label=\"Story\")\n",
        "            \n",
        "            with gr.Row():\n",
        "                image_display = gr.Image(label=\"Illustration\", type=\"filepath\", visible=False)\n",
        "            \n",
        "            audio_display = gr.Audio(label=\"Narration\", type=\"filepath\", autoplay=True)\n",
        "\n",
        "    # Event Handlers\n",
        "    \n",
        "    # Streaming Event\n",
        "    webcam_input.stream(\n",
        "        fn=process_emotion_stream,\n",
        "        inputs=[webcam_input, timer_state],\n",
        "        outputs=[emotion_state, timer_state],\n",
        "        show_progress=False\n",
        "    )\n",
        "\n",
        "    start_btn.click(\n",
        "        fn=start_story_handler,\n",
        "        inputs=[theme_input, lang_input, state],\n",
        "        outputs=[story_display, audio_display, image_display, state, moral_info, status_info]\n",
        "    )\n",
        "    \n",
        "    continue_btn.click(\n",
        "        fn=continue_story_handler,\n",
        "        inputs=[choice_input, emotion_state, state],\n",
        "        outputs=[story_display, audio_display, image_display, state, moral_info, status_info]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting Web Server at http://127.0.0.1:7860...\")\n",
        "    demo.launch(theme=gr.themes.Soft(), quiet=True) # quiet to suppress some Gradio logs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "084a9b08",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
